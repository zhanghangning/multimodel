# -*- coding: utf-8 -*-
# main.py

import yaml
import torch
import argparse
from train_eval import train_model, evaluate
# main.py

import yaml
import torch
import argparse
from train_eval import train_model, evaluate
from nas_search import train_nas
from multimodal_model import MultimodalModel, ImageEncoder, TextEncoder, AudioEncoder, MultimodalFusion
from data_loader import RealImageTextAudioDataset
from torch.utils.data import DataLoader

import os
os.environ["TRANSFORMERS_OFFLINE"] = "1"
os.environ["HF_DATASETS_OFFLINE"] = "1"

def load_config(config_path):
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    print("ğŸ”§ Loaded Config:\n", config)  # æ‰“å°æ•´ä¸ª config æŸ¥çœ‹æ˜¯å¦è¿˜æœ‰å…¶ä»–ç±»å‹é”™è¯¯
    config['device'] = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    return config

def parse_args():
    """
    è§£æå‘½ä»¤è¡Œå‚æ•°
    """
    parser = argparse.ArgumentParser(description="Run Multi-Modal NAS and Training Pipeline")

    parser.add_argument("--config", type=str, default="configs/default.yaml",
                        help="Path to config YAML file")

    parser.add_argument("--mode", choices=["nas", "train", "all"], default="all",
                        help="Mode: 'nas' for architecture search, 'train' for retraining, 'all' for both")

    return parser.parse_args()


def run_nas(config):
    """
    è¿è¡Œ NAS æœç´¢æµç¨‹
    """
    print("ğŸš€ å¼€å§‹è¿›è¡Œ NAS æ¶æ„æœç´¢...")

    # åŠ è½½çœŸå®æ•°æ®é›†
    dataset = RealImageTextAudioDataset(
        coco_ann_file=config['dataset']['coco_ann_file'],
        image_dir=config['dataset']['image_dir'],
        audio_dir=config['dataset']['audio_dir'],
        categories=config['dataset'].get('categories', ["dog", "cat", "car", "airplane", "person"]),
        max_per_class=config['dataset'].get('max_per_class', 100)
    )
    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [
        int(0.8 * len(dataset)), len(dataset) - int(0.8 * len(dataset))
    ])
    train_loader = DataLoader(train_dataset, batch_size=config['train']['batch_size'], shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=config['train']['batch_size'])

    # è®­ç»ƒ NAS æ§åˆ¶å™¨
    controller = train_nas(config)

    # å¯è§†åŒ–æœ€ä½³æ¶æ„
    visualize_best_architecture(config)

    # è¿”å›æ§åˆ¶å™¨ä»¥ä¾›åç»­è®­ç»ƒä½¿ç”¨
    return controller


def visualize_best_architecture(config):
    """
    å¯è§†åŒ– NAS æœç´¢åˆ°çš„æœ€ä½³æ¶æ„
    """
    from nas_search import EnhancedController

    device = config['device']
    controller = EnhancedController(num_ops=config['model']['num_ops'])
    controller.load_state_dict(torch.load('checkpoints/nas_controller.pth'))
    controller = controller.to(device)

    with torch.no_grad():
        best_image_op, best_text_op, best_audio_op = controller.sample(1, device)
    best_image_op = best_image_op.item()
    best_text_op = best_text_op.item()
    best_audio_op = best_audio_op.item()

    print(f"\nğŸ“Š æœ€ä½³æ¶æ„å¯è§†åŒ–ï¼š\n"
          f"ğŸ–¼ï¸ å›¾åƒç¼–ç æ“ä½œ ID: {best_image_op}\n"
          f"ğŸ“ æ–‡æœ¬ç¼–ç æ“ä½œ ID: {best_text_op}\n"
          f"ğŸ”Š éŸ³é¢‘ç¼–ç æ“ä½œ ID: {best_audio_op}")


def run_training_with_best_architecture(config, controller=None):
    """
    ä½¿ç”¨ NAS æœç´¢åˆ°çš„æœ€ä½³æ¶æ„è¿›è¡Œæ¨¡å‹è®­ç»ƒ
    """
    print("ğŸš€ ä½¿ç”¨æœ€ä½³æ¶æ„å¼€å§‹è®­ç»ƒå¤šæ¨¡æ€æ¨¡å‹...")

    device = config['device']

    # å¦‚æœæœªæä¾›æ§åˆ¶å™¨ï¼Œåˆ™åŠ è½½å·²ä¿å­˜çš„ NAS æ§åˆ¶å™¨æƒé‡
    if controller is None:
        from nas_search import EnhancedController
        controller = EnhancedController(num_ops=config['model']['num_ops'])
        controller.load_state_dict(torch.load('checkpoints/nas_controller.pth'))
        controller = controller.to(device)

    # è·å–æœ€ä½³æ¶æ„
    with torch.no_grad():
        best_image_op, best_text_op, best_audio_op = controller.sample(1, device)
    best_image_op = best_image_op.item()
    best_text_op = best_text_op.item()
    best_audio_op = best_audio_op.item()

    print(f"ğŸ† æœ€ä½³æ¶æ„ï¼š"
          f"Image Op={best_image_op}, "
          f"Text Op={best_text_op}, "
          f"Audio Op={best_audio_op}")

    # åˆå§‹åŒ–å¹¶è®­ç»ƒæ¨¡å‹
    model = train_model(config, best_image_op, best_text_op, best_audio_op)

    # æµ‹è¯•æ¨¡å‹æ€§èƒ½
    test_dataset = RealImageTextAudioDataset(**config['test_dataset'])
    test_loader = DataLoader(test_dataset, batch_size=config['train']['batch_size'])

    best_model = MultimodalModel(
        ImageEncoder(num_ops=config['model']['num_ops']),
        TextEncoder(input_dim=config['dataset'].get('text_dim', 768)),
        AudioEncoder(input_dim=config['dataset'].get('audio_dim', 40)),
        MultimodalFusion(**config['fusion'])
    ).to(device)

    best_model.load_state_dict(torch.load('checkpoints/best_model.pth'))

    test_acc = evaluate(config, best_model, test_loader, best_image_op, best_text_op, best_audio_op)
    print(f"\nğŸ æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {test_acc:.2f}%")


if __name__ == "__main__":
    args = parse_args()
    config = load_config(args.config)

    device = config['device']
    print(f"Using device: {device}")

    controller = None

    if args.mode in ["nas", "all"]:
        controller = run_nas(config)

    if args.mode in ["train", "all"]:
        run_training_with_best_architecture(config, controller)